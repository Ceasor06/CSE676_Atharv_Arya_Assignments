# -*- coding: utf-8 -*-
"""CSE676_Ass_1.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1hDZqbgzbeNwuOsx_zWPxgeOKZ-pxnR7M
"""

! pip install torchvision

import torch
import torchvision
import torch.utils.data
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
from torchvision import transforms
from sklearn.model_selection import train_test_split
from torch.utils.data import DataLoader
import matplotlib.pyplot as plt

import torchvision.datasets as datasets

transform = transforms.Compose([transforms.ToTensor(),
                              transforms.Normalize((0.5,), (0.5,)),
                              ])

trainset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)
testset = datasets.MNIST(root='./data', train=False, download=True, transform=transform)
trainloader = torch.utils.data.DataLoader(trainset, batch_size = 64, shuffle=True)
testloader = torch.utils.data.DataLoader(testset, batch_size = 64, shuffle=False)

dataiter = iter(trainloader)
images, labels = next(dataiter)#dataiter.__next__()

print(images.shape)
print(labels.shape)

figure = plt.figure()
imgs = 70
num_img = min(imgs, len(images))
for i in range(1, num_img +1):
  plt.subplot(7,10, i)
  plt.axis('off')
  plt.imshow(images[i-1].numpy().squeeze(), cmap='gray_r');

"""# Model"""

class FNN(nn.Module):
  def __init__(self):
    super().__init__()
    self.layer1 = nn.Linear(28*28, 128) # input layer of size of 784
    self.layer2 = nn.Linear(128, 64) # 1st hidden layer of 128
    self.layer3 = nn.Linear(64, 10) # 2nd hidden layer of input size 64, with output of 10

  def forward(self, X):
    X = X.view(-1, 28*28)
    X = F.relu(self.layer1(X))
    X = F.relu(self.layer2(X))
    X = self.layer3(X)
    return F.log_softmax(X, dim=1) # Output

fnn = FNN()

print(fnn)

criterion = nn.CrossEntropyLoss()
optimizer = optim.Adam(fnn.parameters())

train_losses = []
test_losses = []
iterations = []

iteration = 0
for epoch in range(5):
    fnn.train()
    loss_F = 0.0
    for i, data in enumerate(trainloader, 0):
        inputs, labels = data
        optimizer.zero_grad()
        outputs = fnn(inputs)
        loss = criterion(outputs, labels)
        loss.backward()
        optimizer.step()

        loss_F += loss.item()
        iteration +=1
    avg_train_loss = loss_F / len(trainloader)
    train_losses.append(avg_train_loss)
    iterations.append(epoch+1)

    fnn.eval()
    test_loss = 0.0
    with torch.no_grad():
        for data in testloader:
            inputs, labels = data
            outputs = fnn(inputs)
            loss = criterion(outputs, labels)
            test_loss += loss.item()

    avg_test_loss = test_loss / len(testloader)
    test_losses.append(avg_test_loss)

    print(f'Epoch {epoch+1}/{5} - Train Loss: {avg_train_loss:.4f}, Test Loss: {avg_test_loss:.4f}')

plt.figure(figsize=(10, 6))
plt.plot(iterations, train_losses, label='Training Loss')
plt.plot(iterations, test_losses, label='Test Loss', linestyle='--')
plt.xlabel('Iterations')
plt.ylabel('Loss')
plt.title('Training and Test Loss vs Iterations')
plt.legend()
plt.show()

"""# References:
1. https://www.datascienceweekly.org/tutorials/pytorch-mnist-load-mnist-dataset-from-pytorch-torchvision#:~:text=Brilliant%20%2D%20We%20were%20able%20to,dataset%20and%20a%20test%20dataset.

2. https://towardsdatascience.com/handwritten-digit-mnist-pytorch-977b5338e627

3. https://www.kaggle.com/code/sdelecourt/cnn-with-pytorch-for-mnist
"""

